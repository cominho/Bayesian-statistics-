---
title: "Comparaison between BART, DART and Random Forest"
output: html_document
---

```{r setup, include=FALSE}
library(BayesTree)
library(randomForest)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE, warning = TRUE, message = TRUE)

# Setting
options(
  error = function() {
    traceback(2)
    quit(save = "no", status = 1)
  },
  warn = 1
)
```

## Data generation

```{r data-generation}
# Sparse function
sparse_function <- function(x) {
  10 * sin(pi * x[, 1] * x[, 2]) +
    20 * (x[, 3] - 0.5)^2 +
    10 * x[, 4] +
    5 * x[, 5]
}

generate_sparse_data <- function(n, P, noise_sd = 1) {
  if (P < 5) stop("The function needs at least 5 columns to be valid.")
  x <- matrix(runif(n * P, min = 0, max = 1), nrow = n, ncol = P)
  y <- sparse_function(x) + rnorm(n, sd = noise_sd)
  return(list(x = x, y = y))
}

# Generate data 
n <- 100  
P_values <- c(10, 50, 90)
datasets <- lapply(P_values, function(P) generate_sparse_data(n, P))
names(datasets) <- paste0("P=", P_values)

for (name in names(datasets)) {
  data <- datasets[[name]]
  cat("\n=== Data for", name, "===\n")
  cat("X dimension :", dim(data$x), "\n")
  cat("y size :", length(data$y), "\n")
  print(summary(data$y))
}
```

---

## Implémentation de DART

```{r dart-implementation}
# Ajust DART with a Dirichlet prior 
sample_alpha_prior <- function(P, a = 0.5, b = 1, rho = P) {
  alpha <- rbeta(1, a, b) * rho / (1 - rbeta(1, a, b))
  return(alpha)
}

dart <- function(x.train, y.train, alpha, ntree = 200) {
  # Modify the probabilities
  prior <- function(x) {
    s <- rgamma(ncol(x), shape = alpha / ncol(x), rate = 1)
    s / sum(s)
  }

  # SImulate DART
  bart_fit <- bart(
    x.train = x.train,
    y.train = y.train,
    ntree = ntree,
    keepevery = 5,  
    usequants = TRUE
  )

  # Add prior to the model
  bart_fit$prior <- prior(x.train)
  return(bart_fit)
}

# Count number of variables included in the splits
count_splits <- function(model, n_vars) {
  varcount_matrix <- model$varcount  
  var_usage <- apply(varcount_matrix, 2, mean)  
  selected_vars <- sum(var_usage > 0)  
  return(selected_vars)
}
```

---

## Ajustement des Modèles

```{r models}
run_models <- function(data, alpha) {
  x <- data$x
  y <- data$y

  cat("\n--- Ajustement des Modèles ---\n")

  cat("BART model adjustment...\n")
  bart_fit <- bart(x.train = x, y.train = y, ntree = 200)

  cat("DART model adjustment...\n")
  dart_fit <- dart(x.train = x, y.train = y, alpha = alpha, ntree = 200)

  # Random Forest
  cat("Random Forest model adjustment...\n")
  rf_fit <- randomForest(x, y)

  return(list(bart = bart_fit, dart = dart_fit, rf = rf_fit))
}


alphas <- sapply(P_values, function(P) sample_alpha_prior(P))

results <- mapply(
  function(data, alpha) run_models(data, alpha),
  datasets, alphas,
  SIMPLIFY = FALSE
)
```

---

## Vizualize results

```{r visualisation, fig.height=5, fig.width=7}
for (name in names(results)) {
  fit <- results[[name]]
  bart_fit <- fit$bart
  dart_fit <- fit$dart
  rf_fit <- fit$rf

  true_values <- datasets[[name]]$y
  bart_preds <- bart_fit$yhat.train.mean
  dart_preds <- dart_fit$yhat.train.mean
  rf_preds <- predict(rf_fit, datasets[[name]]$x)

  par(mfrow = c(1, 3))

  # BART
  plot(true_values, bart_preds, 
       main = paste("BART -", name), 
       xlab = "True values", ylab = "Predictions",
       col = "blue", pch = 16)
  abline(0, 1, col = "red")

  # DART
  plot(true_values, dart_preds, 
       main = paste("DART -", name), 
       xlab = "True values", ylab = "Predictions",
       col = "purple", pch = 16)
  abline(0, 1, col = "red")

  # Random Forest
  plot(true_values, rf_preds, 
       main = paste("Random Forest -", name), 
       xlab = "True values", ylab = "Predictions",
       col = "green", pch = 16)
  abline(0, 1, col = "red")
}
```

---

## Residuals analysis

```{r residuals-boxplot}
par(mfrow = c(1, length(results)))
for (name in names(results)) {
  fit <- results[[name]]
  bart_fit <- fit$bart
  dart_fit <- fit$dart
  rf_fit <- fit$rf

  true_values <- datasets[[name]]$y
  bart_residuals <- true_values - bart_fit$yhat.train.mean
  dart_residuals <- true_values - dart_fit$yhat.train.mean
  rf_residuals <- true_values - predict(rf_fit, datasets[[name]]$x)

  boxplot(
    bart_residuals, dart_residuals, rf_residuals,
    names = c("BART", "DART", "RF"),
    main = paste("Residuals -", name),
    col = c("blue", "purple", "green")
  )
}
```


## Sparsité et Inclusion des Variables avec Visualisation
```{r sparsity-analysis, echo=TRUE, warning=FALSE, message=FALSE, fig.height=5, fig.width=7}

# Résultats de sparsité
results_alpha <- data.frame()
alpha_values <- c(0.1, 1, 10)

for (alpha in alpha_values) {
  for (P in P_values) {
    data <- generate_sparse_data(n = 100, P = P)
    dart_fit <- dart(x.train = data$x, y.train = data$y, alpha = alpha)
    Q <- count_splits(dart_fit, P)
    results_alpha <- rbind(results_alpha, data.frame(P = P, alpha = alpha, Q = Q))
  }
}


ggplot(results_alpha, aes(x = P, y = Q, group = factor(alpha), color = factor(alpha))) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  labs(
    title = "Impact of alpha and P on sparsity",
    x = "Number of predictors (P)",
    y = "Number of variables (Q)",
    color = "Alpha value"
  ) +
  theme_minimal()
```